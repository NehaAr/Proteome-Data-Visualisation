{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rm6eNF1fB5j9",
        "outputId": "211ba13b-e6c8-4433-d368-040652f85cc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: adjustText in /usr/local/lib/python3.8/dist-packages (0.7.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from adjustText) (3.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from adjustText) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->adjustText) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->adjustText) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->adjustText) (1.4.4)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->adjustText) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->adjustText) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting Bio\n",
            "  Downloading bio-1.5.3-py3-none-any.whl (272 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m272.6/272.6 KB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from Bio) (2.25.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from Bio) (4.64.1)\n",
            "Collecting biopython>=1.80\n",
            "  Downloading biopython-1.80-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mygene\n",
            "  Downloading mygene-3.2.2-py2.py3-none-any.whl (5.4 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from biopython>=1.80->Bio) (1.21.6)\n",
            "Collecting biothings-client>=0.2.6\n",
            "  Downloading biothings_client-0.2.6-py2.py3-none-any.whl (37 kB)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->Bio) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->Bio) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->Bio) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->Bio) (4.0.0)\n",
            "Installing collected packages: biopython, biothings-client, mygene, Bio\n",
            "Successfully installed Bio-1.5.3 biopython-1.80 biothings-client-0.2.6 mygene-3.2.2\n"
          ]
        }
      ],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "#define helping function\n",
        "!pip install adjustText\n",
        "!pip install Bio\n",
        "import os\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import MDS\n",
        "from adjustText import adjust_text\n",
        "from matplotlib.lines import Line2D\n",
        "from Bio import SeqIO\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy.stats import ttest_ind\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import missingno as msno\n",
        "import matplotlib\n",
        "import inspect, re\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "def namestr(obj, namespace):\n",
        "    return [name for name in namespace if namespace[name] is obj][0]\n",
        "\n",
        "def compare_sets(s1=set(),s2=set(),name1='s1',name2='s2'):\n",
        "    common = len(set(s1) & set(s2))\n",
        "    uS1 = len(set(s1) - set(s2))\n",
        "    uS2 = len(set(s2) - set(s1))\n",
        "    res = pd.DataFrame(columns=[name1,name2])\n",
        "    res.loc['size',:]=[len(s1),len(s2)]\n",
        "    res.loc['common',:]=[common,common]\n",
        "    res.loc['unique',:]=[uS1,uS2]\n",
        "    str_report=\"\"\"\n",
        "    {lenS1} in {s1}\n",
        "    {lenS2} in {s2} \n",
        "    {common} in common\n",
        "    {uS1} unique {s1}\n",
        "    {uS2} unique in {s2}     \n",
        "    \"\"\".format(\n",
        "        s1 = res.columns[0],\n",
        "        s2 = res.columns[1],\n",
        "        lenS1 = res.loc['size',res.columns[0]],\n",
        "        lenS2 = res.loc['size',res.columns[1]],\n",
        "        common=res.loc['common',res.columns[0]],\n",
        "        uS1 = res.loc['unique',res.columns[0]],\n",
        "        uS2 = res.loc['unique',res.columns[1]],\n",
        "              )\n",
        "    return str_report,res\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def quantileNormalize(df_input, keep_na=True):\n",
        "    df = df_input.copy()\n",
        "    #compute rank\n",
        "    dic = {}\n",
        "    for col in df:\n",
        "        dic.update({col : sorted(df[col])})\n",
        "    sorted_df = pd.DataFrame(dic)\n",
        "    rank = sorted_df.mean(axis = 1).tolist()\n",
        "    #sort\n",
        "    for col in df:\n",
        "        t = np.searchsorted(np.sort(df[col]), df[col])\n",
        "        norm = [rank[i] for i in t]\n",
        "        if keep_na == True:\n",
        "            norm = [np.nan if np.isnan(a) else b for a,b in zip(df[col],norm)]\n",
        "        df[col] =  norm             \n",
        "    return df\n",
        "\n",
        "\n",
        "#get only the gene id from\n",
        "#the new TryTripDB format\n",
        "def clean_id(temp_id):\n",
        "    temp_id = temp_id.split(':')[0]\n",
        "    if temp_id.count('.')>2:\n",
        "        temp_id = '.'.join(temp_id.split('.')[0:3])\n",
        "    return temp_id\n",
        "\n",
        "#helper function to print out\n",
        "#the protein removed at each threshold\n",
        "def print_result(start_df_shape, shape_before, df, what):\n",
        "    removed = shape_before[0]- df.shape[0]\n",
        "    removed_from_beginning = start_df_shape[0]-df.shape[0]\n",
        "    if removed > 0:\n",
        "        print ('removed ',removed, 'Protein Groups by:', what )  \n",
        "        print ('tot ', removed_from_beginning, ' entries removed' )\n",
        "        print ('---------------')\n",
        "    else:\n",
        "        print (what)\n",
        "        print ('nothing removed')\n",
        "        print ('---------------')\n",
        "\n",
        "#remove rubbish entires from a\n",
        "#maxquant output\n",
        "def clean_df(df, id_by_site=True, rev_database=True, \n",
        "             contaminant=True, score=False, unique_pep_threshold=False):  \n",
        "    before,start = df.shape,df.shape\n",
        "    print('starting from:', before)\n",
        "    if id_by_site:\n",
        "        #remove Only identified by site\n",
        "        before,start = df.shape,df.shape\n",
        "        col = 'Only identified by site'\n",
        "        df = df[df[col] != '+'] \n",
        "        print_result(start, before, df, col)\n",
        "    \n",
        "    if rev_database:\n",
        "        #remove hits from reverse database\n",
        "        before = df.shape\n",
        "        col = 'Reverse'\n",
        "        df = df[df[col] != '+']\n",
        "        print_result(start, before, df, col)\n",
        "     \n",
        "\n",
        "    if contaminant:\n",
        "        #remove contaminants (mainly keratine and bsa)\n",
        "        before = df.shape\n",
        "        col = 'Potential contaminant'\n",
        "        df = df[df[col] != '+']\n",
        "        print_result(start, before, df, col)\n",
        "\n",
        "    if score:\n",
        "        before = df.shape\n",
        "        col = 'Score'\n",
        "        df = df[df[col] >= score]\n",
        "        print_result(start, before, df, col)\n",
        "        \n",
        "    if unique_pep_threshold:    \n",
        "    ##remove protein groups with less than unique peptides\n",
        "        before = df.shape\n",
        "        col = 'Peptide counts (razor+unique)'\n",
        "        df['unique_int'] = [int(n.split(';')[0]) for n in df[col]]\n",
        "        df = df[df['unique_int'] >= unique_pep_threshold]\n",
        "        print_result(start, before, df, col)\n",
        "    return df  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#extract the description from the fasta headers\n",
        "#of the proten group\n",
        "def make_desc(n, lookfor='gene_product'):\n",
        "    temp_dict = {}\n",
        "    n=str(n)\n",
        "    if 'gene-Tb427' in n:\n",
        "        desc = n.split(' ')[0].replace('gene-Tb427.','')\n",
        "        return desc\n",
        "    \n",
        "    if 'sp|' in n:\n",
        "        item_list = n.split(';')\n",
        "        desc = []\n",
        "        for n in item_list[0].split(' '):\n",
        "            if '=' not in n and 'sp|' not in n:\n",
        "                desc.append(n)\n",
        "            if '=' in n:\n",
        "                break\n",
        "        desc = ' '.join(desc)\n",
        "        return desc\n",
        "           \n",
        "    item_list = n.split(' | ')\n",
        "    for n in item_list:\n",
        "        if '=' in n:\n",
        "            key = n.split('=')[0].strip()\n",
        "            value= n.split('=')[1].strip()\n",
        "            temp_dict[key]=value\n",
        "\n",
        "                  \n",
        "    return temp_dict.get(lookfor,'none')\n",
        "\n",
        "\n",
        "#rename some of maxquant output \n",
        "#columns\n",
        "def mod_df(df, desc_from_id=False, desc_value='gene_product', id_cols = 'Protein IDs' ):\n",
        "    df['Gene_id'] = [clean_id(n.split(':')[0].split(';')[0])\n",
        "                     for n in df[id_cols]]\n",
        "    df['desc'] = df['Fasta headers'].apply(make_desc, lookfor=desc_value)\n",
        "    return df\n",
        "   \n",
        "\n",
        "#create a dictionary id -> description\n",
        "#from a trytripdb fasta file\n",
        "def make_desc_dict(path_to_file):\n",
        "    desc_dict = {}\n",
        "    with open(path_to_file, \"r\") as handle:\n",
        "        a=0\n",
        "        for record in SeqIO.parse(handle, \"fasta\"):\n",
        "            a+=1\n",
        "            temp_id = clean_id(record.id).strip()\n",
        "            temp_desc = record.description.split('|')[4].strip()\n",
        "            desc_dict[temp_id]=temp_desc\n",
        "    return desc_dict\n",
        "\n",
        "\n",
        "\n",
        "                           \n",
        "#make pca plot from pandas df\n",
        "def make_pca(in_df, palette, ax, top=500, \n",
        "             color_dictionary=False, do_adjust_text=False):\n",
        "    \n",
        "    cols = in_df.columns\n",
        "    \n",
        "    \n",
        "    sorted_mean = in_df.mean(axis=1).sort_values()\n",
        "    select = sorted_mean.tail(top)\n",
        "    #print(top)\n",
        "    in_df = in_df.loc[select.index.values]\n",
        "    \n",
        "    pca = PCA(n_components=2)\n",
        "    pca.fit(in_df)\n",
        "    \n",
        "    temp_df = pd.DataFrame()\n",
        "    temp_df['pc_1']=pca.components_[0]\n",
        "    temp_df['pc_2']=pca.components_[1]\n",
        "    temp_df.index = cols\n",
        "    print(pca.explained_variance_ratio_)\n",
        "    temp_df['color']=palette\n",
        "    #fig,ax=plt.subplots(figsize=(12,6))\n",
        "    temp_df.plot(kind='scatter',x='pc_1', y='pc_2',s=30, c=temp_df['color'], ax=ax)\n",
        "    #print(temp_df.index.values)\n",
        "\n",
        "    for color in temp_df['color'].unique():\n",
        "        c_data = temp_df[temp_df['color']==color].iloc[0]\n",
        "        ax.scatter(x=c_data.pc_1, y=c_data.pc_2, c=color, label=color,s=30)\n",
        "        ax.legend(title='Groups',loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "    \n",
        "    \n",
        "    \n",
        "    texts = [ax.text(temp_df.iloc[i]['pc_1'], \n",
        "                       temp_df.iloc[i]['pc_2'],\n",
        "                       cols[i])\n",
        "                       for i in range(temp_df.shape[0])]\n",
        "    \n",
        "    if do_adjust_text:\n",
        "        adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'),ax=ax)\n",
        "    ax.set_title('PCA', size=14)\n",
        "    ax.set_xlabel('PC1_{:.3f}'.format(pca.explained_variance_ratio_[0]),size=12)\n",
        "    ax.set_ylabel('PC2_{:.3f}'.format(pca.explained_variance_ratio_[1]),size=12)\n",
        "    ax.yaxis.label.set_size(12)\n",
        "    ax.xaxis.label.set_size(12)\n",
        "    \n",
        "    if color_dictionary:\n",
        "        print(color_dictionary)\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        labels = [ color_dictionary[l] for l in labels]\n",
        "        ax.legend(handles=handles, labels=labels, \n",
        "        title='Groups',loc='center left', bbox_to_anchor=(1, 0.9))    \n",
        "    return ax\n",
        "#make mds plot from pandas df  \n",
        "def make_mds(in_df, palette, ax, top=500, \n",
        "             color_dictionary=False,do_adjust_text=True):\n",
        "    cols = in_df.columns\n",
        "    \n",
        "    \n",
        "    \n",
        "    sorted_mean = in_df.mean(axis=1).sort_values()\n",
        "    select = sorted_mean.tail(top)\n",
        "    #print(top)\n",
        "    in_df = in_df.loc[select.index.values]\n",
        "    \n",
        "    pca = MDS(n_components=2, metric=True)\n",
        "    temp_df = pd.DataFrame(pca.fit_transform(in_df.T),\n",
        "                                 index=cols, \n",
        "                           columns =['pc_1', 'pc_2'] )\n",
        "    \n",
        "    temp_df['color']=palette\n",
        "    \n",
        "    temp_df.plot(kind='scatter',x='pc_1', y='pc_2', s=50, c=temp_df['color'], ax=ax)\n",
        "    #print(temp_df.head())\n",
        "    for color in temp_df['color'].unique():\n",
        "        c_data = temp_df[temp_df['color']==color].iloc[0]\n",
        "        ax.scatter(x=c_data.pc_1, y=c_data.pc_2, c=color, label=color,s=50)\n",
        "        ax.legend(title='Groups',loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "      \n",
        "    \n",
        "    #.plot(kind='scatter',x='pc_1', y='pc_2', s=50, c=temp_df['color'], ax=ax)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    texts = [ax.text(temp_df.iloc[i]['pc_1'], \n",
        "                       temp_df.iloc[i]['pc_2'],\n",
        "                       cols[i])\n",
        "                       for i in range(temp_df.shape[0])]\n",
        "    if do_adjust_text:\n",
        "        adjust_text(texts, arrowprops=dict(arrowstyle='->', color='red'),ax=ax)\n",
        "    ax.set_title('MDS',size=14)\n",
        "    ax.set_xlabel('DIM_1',size=12)\n",
        "    ax.set_ylabel('DIM_2',size=12)\n",
        "    ax.yaxis.label.set_size(12)\n",
        "    ax.xaxis.label.set_size(12)\n",
        "    if color_dictionary:\n",
        "        print(color_dictionary)\n",
        "        handles, labels = ax.get_legend_handles_labels()\n",
        "        labels = [ color_dictionary[l] for l in labels]\n",
        "        ax.legend(handles=handles, labels=labels, \n",
        "        title='Groups',loc='center left', bbox_to_anchor=(1, 0.9))\n",
        "    return ax\n",
        "\n",
        "\n",
        "#format legend of hist plots \n",
        "#with lines instead of boxes\n",
        "def hist_legend(ax, title = False):\n",
        "    handles, labels = ax.get_legend_handles_labels()\n",
        "    new_handles = [Line2D([], [], c=h.get_edgecolor()) for h in handles]\n",
        "    ax.legend(handles=new_handles, labels=labels, \n",
        "    title=title,loc='center left', bbox_to_anchor=(1, 0.5))  \n",
        "\n",
        "\n",
        "#get a random distribution of numbers \n",
        "#around the minimum value \n",
        "#of a columns (greather than zero)\n",
        "#with small std\n",
        "def get_random(in_col, strategy):\n",
        "    if strategy == 'small':\n",
        "        mean_random = in_col[in_col>0].min()\n",
        "        std_random = mean_random*0.25\n",
        "        random_values = np.random.normal(mean_random, \n",
        "                                         scale=std_random, \n",
        "                                         size=in_col.shape[0])\n",
        "    if strategy == 'median':\n",
        "        pass\n",
        "        \n",
        "    return  random_values\n",
        "\n",
        "#add a small random value to each element\n",
        "#of a cloumn, optionally plots the distribution\n",
        "#of the random values\n",
        "def impute(in_col, ax=False, strategy='small'):\n",
        "    random_values = get_random(in_col, strategy=strategy)\n",
        "    if ax:\n",
        "        np.log10(pd.Series(random_values)).plot(kind='hist',histtype='step', \n",
        "                          density=True,ax=ax,label=in_col.name)  \n",
        "    \n",
        "    fake_col = in_col.copy()\n",
        "    fake_col = fake_col+random_values\n",
        "    index = in_col[in_col==0].index.values \n",
        "    in_col.loc[index] = fake_col.loc[index] \n",
        "    return in_col    \n",
        "\n",
        "#replace missing values with zeros\n",
        "def replace_nan(col):\n",
        "    col = col.replace('NaN', np.nan)\n",
        "    col = col.fillna(0)\n",
        "    return col\n",
        "\n",
        "#normalization of dataframe\n",
        "#to account for uneven loading\n",
        "def norm_loading_TMT(df):\n",
        "    col_sum = df.sum(axis=0)\n",
        "    #print(col_sum)\n",
        "    target = np.mean(col_sum)\n",
        "    #print(target)\n",
        "    norm_facs = target / col_sum\n",
        "    #print(norm_facs)\n",
        "    data_norm = df.multiply(norm_facs, axis=1)\n",
        "    return  data_norm\n",
        "\n",
        "def norm_loading(df):\n",
        "    col_sum = df.median(axis=0)\n",
        "    print(col_sum)\n",
        "    target = np.mean(col_sum)\n",
        "    print(target)\n",
        "    norm_facs = target / col_sum\n",
        "    print(norm_facs)\n",
        "    data_norm = df.multiply(norm_facs, axis=1)\n",
        "    return data_norm\n",
        "\n",
        "#essentially a scatter plot with the option \n",
        "#of annootating group of genes \n",
        "def make_vulcano(df, ax, x='-Log10PValue', \n",
        "                 y='Log2FC',\n",
        "                 fc_col = 'Log2FC',\n",
        "                 fc_limit=False,\n",
        "                 \n",
        "                 pval_col = 'PValue',\n",
        "                 pval_limit=False,\n",
        "                 \n",
        "                 annot_index=pd.Series(), \n",
        "                 annot_names=pd.Series(),\n",
        "                 title='Volcano',\n",
        "                 legend_title='',\n",
        "                 \n",
        "                 label_for_selection = None,\n",
        "                 label_for_all = None,\n",
        "                 add_text = True,\n",
        "                 do_adjust_text=True,\n",
        "                 text_size = 8,\n",
        "                 rolling_mean = False,\n",
        "                 alpha_main=0.05,\n",
        "                point_size_selection=1,\n",
        "                point_size_all=1,\n",
        "                fontdict=None,\n",
        "                expand_text=None,\n",
        "                force_text=None,\n",
        "                expand_points=None):\n",
        "    \n",
        "\n",
        "    if fc_limit and pval_limit:\n",
        "        upper = df[df[fc_col]>fc_limit].copy()\n",
        "        lower = df[df[fc_col]<-fc_limit].copy()\n",
        "        \n",
        "        upper = upper[upper[pval_col]<pval_limit]\n",
        "        lower = lower[lower[pval_col]<pval_limit]\n",
        "         \n",
        "    elif pval_limit:\n",
        "        upper = df[df[pval_col]<pval_limit].copy()\n",
        "        lower = df[df[pval_col]<pval_limit].copy()\n",
        "        \n",
        "    elif fc_limit:\n",
        "        upper = df[df[fc_col]>fc_limit].copy()\n",
        "        lower = df[df[fc_col]<-fc_limit].copy()\n",
        "\n",
        "    else: \n",
        "        print('no selection')    \n",
        "            \n",
        "    \n",
        "    to_remove = []\n",
        "    if 'upper' in locals() and upper.shape[0]>0:\n",
        "        #print(upper.head())\n",
        "        upper.plot(\n",
        "        kind='scatter',x=x,y=y, ax=ax, \n",
        "        c='r', label='Bigger Than {fc_limit}'.format(fc_limit=fc_limit), alpha=0.5, zorder=5)\n",
        "        to_remove.append(upper)\n",
        "        \n",
        "    if 'lower' in locals() and lower.shape[0]>0:     \n",
        "        lower.plot(\n",
        "        kind='scatter',x=x,y=y, ax=ax, \n",
        "        c='g', label='Lower Than {fc_limit}'.format(fc_limit=fc_limit), alpha=0.5, zorder=5)       \n",
        "        to_remove.append(lower)\n",
        "\n",
        "\n",
        "    if len(annot_index) > 0:\n",
        "        df.loc[annot_index].plot(kind='scatter', x=x, y=y, c='r', \n",
        "                                 s=point_size_selection, ax=ax, \n",
        "                                 label=label_for_selection, alpha=1, zorder=10)\n",
        "        to_remove.append(df.loc[annot_index])\n",
        "                \n",
        "\n",
        " \n",
        "\n",
        "    \n",
        "    if len(to_remove)>0:\n",
        "        to_remove=pd.concat(to_remove)\n",
        "        idx = df.index.difference(to_remove.index)\n",
        "        df.loc[idx].plot(kind='scatter', x=x, y=y, ax=ax, \n",
        "                         alpha=alpha_main,c='b', zorder=1, label=label_for_all,\n",
        "                        s=point_size_all)\n",
        "    else:\n",
        "        df.plot(kind='scatter', x=x, y=y, ax=ax, \n",
        "                alpha=alpha_main,c='b', zorder=1,label=label_for_all,s=point_size_all)\n",
        "    \n",
        "    if rolling_mean:\n",
        "        df = df.sort_values(x,ascending=False)\n",
        "        df['rolling_mean'] = df[y].rolling(100).mean()\n",
        "        print(df.head())\n",
        "        temp = df[['rolling_mean',x]]\n",
        "        temp=temp.dropna()\n",
        "        temp.plot(ax=ax, x=x, y='rolling_mean', label = 'rolling mean', c='r',alpha=0.3)\n",
        "    \n",
        "        ax.set_xlim(df[x].min()-df[x].min()*0.01,\n",
        "                 df[x].max()+df[x].min()*0.01)\n",
        "\n",
        "\n",
        "    if add_text:\n",
        "        texts = [ax.text(df.loc[i][x], df.loc[i][y],name, fontsize=text_size,fontdict=fontdict)\n",
        "                               for i,name in zip(annot_index,annot_names)]\n",
        "        #print(texts)\n",
        "        if do_adjust_text:\n",
        "            #print('adjusting text')\n",
        "            if not expand_text:\n",
        "                expand_text=(1.1, 1.1)\n",
        "            if not force_text:\n",
        "                force_text=(0.1, 0.2)\n",
        "            if not expand_points:\n",
        "                expand_points=(1.05, 1.2)\n",
        "\n",
        "            adjust_text(texts, arrowprops=dict(arrowstyle='-',\n",
        "                                               color='red',lw=0.8),\n",
        "                                               force_text=force_text,\n",
        "                                               va='bottom',\n",
        "                                               lim=1000,\n",
        "                                               expand_text=expand_text,\n",
        "                                               autoalign='xy',\n",
        "                                               #only_move={'points':'x', 'text':'x'},\n",
        "                        ax=ax)\n",
        "\n",
        "    \n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.8, 0.8), title=legend_title)\n",
        "    \n",
        "    ax.set_title(title)\n",
        "    ax.yaxis.label.set_size(12)\n",
        "    ax.xaxis.label.set_size(12)\n",
        "    return ax\n",
        "\n",
        "\n",
        "\n",
        "#helper function to visualize the correlation between experiments\n",
        "def plot_correlation(df, figname='corr_prot'):\n",
        "    #function to annotate the axes with\n",
        "    #the pearson correlation coefficent\n",
        "    def corrfunc(x, y, **kws):\n",
        "        corr = np.corrcoef(x, y)\n",
        "        r = corr[0][1]\n",
        "        ax = plt.gca()\n",
        "        ax.annotate(\"p = {:.2f}\".format(r),\n",
        "                    xy=(.1, .9), xycoords=ax.transAxes)\n",
        "    \n",
        "    #prepare the seaborn grid and plot\n",
        "    g = sns.PairGrid(df.dropna(), palette=[\"red\"], height=1.8, aspect=1.5)\n",
        "    g.map_upper(plt.scatter, s=5)\n",
        "    g.map_diag(sns.distplot, kde=False)\n",
        "    g.map_lower(sns.kdeplot, cmap=\"Blues_d\")\n",
        "    g.map_lower(corrfunc)\n",
        "    sns.set(font_scale=1.1)\n",
        "    \n",
        "    \n",
        "    \n",
        "def concordance_correlation_coefficient(y_true, y_pred,\n",
        "                       sample_weight=None,\n",
        "                       multioutput='uniform_average'):\n",
        "    \"\"\"Concordance correlation coefficient.\n",
        "    The concordance correlation coefficient is a measure of inter-rater agreement.\n",
        "    It measures the deviation of the relationship between predicted and true values\n",
        "    from the 45 degree angle.\n",
        "    Read more: https://en.wikipedia.org/wiki/Concordance_correlation_coefficient\n",
        "    Original paper: Lawrence, I., and Kuei Lin. \"A concordance correlation coefficient to evaluate reproducibility.\" Biometrics (1989): 255-268.  \n",
        "    Parameters\n",
        "    ----------\n",
        "    y_true : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "        Ground truth (correct) target values.\n",
        "    y_pred : array-like of shape = (n_samples) or (n_samples, n_outputs)\n",
        "        Estimated target values.\n",
        "    Returns\n",
        "    -------\n",
        "    loss : A float in the range [-1,1]. A value of 1 indicates perfect agreement\n",
        "    between the true and the predicted values.\n",
        "    Examples\n",
        "    --------\n",
        "    >>> from sklearn.metrics import concordance_correlation_coefficient\n",
        "    >>> y_true = [3, -0.5, 2, 7]\n",
        "    >>> y_pred = [2.5, 0.0, 2, 8]\n",
        "    >>> concordance_correlation_coefficient(y_true, y_pred)\n",
        "    0.97678916827853024\n",
        "    \"\"\"\n",
        "    cor=np.corrcoef(y_true,y_pred)[0][1]\n",
        "    \n",
        "    mean_true=np.mean(y_true)\n",
        "    mean_pred=np.mean(y_pred)\n",
        "    \n",
        "    var_true=np.var(y_true)\n",
        "    var_pred=np.var(y_pred)\n",
        "    \n",
        "    sd_true=np.std(y_true)\n",
        "    sd_pred=np.std(y_pred)\n",
        "    \n",
        "    numerator=2*cor*sd_true*sd_pred\n",
        "    \n",
        "    denominator=var_true+var_pred+(mean_true-mean_pred)**2\n",
        "\n",
        "    return numerator/denominator\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "class IRS():\n",
        "    \"\"\"\n",
        "     Internal Reference Scaling for multibach TMT\n",
        "     \n",
        "    cols = ['Reporter intensity corrected {}'.format(n) for n in range(0,10)]\n",
        "    experiments = ['E5014','E5015','E5016']\n",
        "    data=df[[b + ' '+ a for a in experiments for b in cols ]]\n",
        "    data.columns  = [str(b) + '_'+ a for a in experiments for b in range(1,11)]\n",
        "    dataIRS =IRS(data=data,\n",
        "                experiments=experiments,\n",
        "                chaneels = range(1,11)) \n",
        "     dataIRS.norm_loading()\n",
        "     dataIRS.norm_irs()\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        data=pd.DataFrame(),\n",
        "        experiments=[],\n",
        "        chaneels = []\n",
        "    ):\n",
        "        self.data= data\n",
        "        self.experiments=experiments\n",
        "        self.chaneels = []\n",
        "        self.columns = []\n",
        "        for e in experiments:\n",
        "            temp = []\n",
        "            for c in chaneels:\n",
        "                temp.append('{c}_{e}'.format(c=c, e=e))\n",
        "            self.columns.append(temp)\n",
        "                    \n",
        "    def norm_loading(self):\n",
        "        data = self.data.copy()\n",
        "        sum_of_columns = []\n",
        "        for cols in self.columns:\n",
        "            #sum of columns for each experiments\n",
        "            sum_of_columns.append(data[cols].sum(axis=0))\n",
        "        target = np.mean(sum_of_columns)\n",
        "        norm_factors = [target / n for n in sum_of_columns]\n",
        "        \n",
        "        for cols, nf in zip(self.columns, norm_factors):\n",
        "            data[cols]=data[cols].multiply(nf, axis=1) \n",
        "        self.data_nl = data\n",
        "        \n",
        "    def norm_irs(self):\n",
        "        data = self.data_nl.copy()\n",
        "        irs = []\n",
        "        for exp, cols in zip(self.experiments, self.columns):\n",
        "            temp = data[cols].sum(axis=1)\n",
        "            temp.name=exp                  \n",
        "            irs.append(temp)\n",
        "        irs=pd.concat(irs,axis=1)\n",
        "        #geometric mean of the sum intensity of all the proteins\n",
        "        irs['average']=np.exp(np.log(irs.replace(0,np.nan)).mean(axis=1))#, skipna=True))\n",
        "        print(irs.head())    \n",
        "        \n",
        "        norm_factors = []\n",
        "        for exp in self.experiments:\n",
        "            norm_factors.append(irs['average'] / irs[exp])\n",
        "        \n",
        "        for cols, nf in zip(self.columns, norm_factors):\n",
        "            data[cols] = data[cols].multiply(nf, axis=0)\n",
        "            \n",
        "        self.data_irs = data\n",
        "        #print(data.head()) \n",
        "\n",
        "class CV():\n",
        "    '''\n",
        "    cols = ['Reporter intensity corrected {}'.format(n) for n in range(0,10)]\n",
        "    experiments = ['E5014','E5015','E5016']\n",
        "    data=df[[b + ' '+ a for a in experiments for b in cols ]]\n",
        "    data.columns  = [str(b) + '_'+ a for a in experiments for b in range(1,11)]\n",
        "    groups = {}\n",
        "    colors = {}\n",
        "    for n in range(1,11):\n",
        "        temp = []\n",
        "        for exp in experiments:\n",
        "            temp.append('{n}_{exp}'.format(n=n,exp=exp))\n",
        "        groups[n]=temp\n",
        "        colors[n]='b'\n",
        "    {1: ['1_E5014', '1_E5015', '1_E5016'],\n",
        "     2: ['2_E5014', '2_E5015', '2_E5016'],\n",
        "     3: ['3_E5014', '3_E5015', '3_E5016'],\n",
        "     4: ['4_E5014', '4_E5015', '4_E5016'],\n",
        "     5: ['5_E5014', '5_E5015', '5_E5016'],\n",
        "     6: ['6_E5014', '6_E5015', '6_E5016'],\n",
        "     7: ['7_E5014', '7_E5015', '7_E5016'],\n",
        "     8: ['8_E5014', '8_E5015', '8_E5016'],\n",
        "     9: ['9_E5014', '9_E5015', '9_E5016'],\n",
        "     10: ['10_E5014', '10_E5015', '10_E5016']}\n",
        "    '''\n",
        "    def __init__(\n",
        "        self,\n",
        "        data,\n",
        "        groups = {},\n",
        "        \n",
        "    ):\n",
        "            self.data = data\n",
        "            self.groups = groups\n",
        "            \n",
        "    \n",
        "    def compute(self):\n",
        "        data = self.data.copy()\n",
        "        cv_means = []\n",
        "        cv_stds = []\n",
        "        cvs = []\n",
        "        groups = self.groups\n",
        "        for group in groups:\n",
        "            #print(group,groups[group])\n",
        "            #if group == 1:\n",
        "               #print(data[groups[group]].head())\n",
        "            temp = data[groups[group]].replace(0,np.nan).mean(axis=1, skipna=True)\n",
        "            cv_means.append(temp)\n",
        "            #print(temp)\n",
        "            temp = data[groups[group]].replace(0,np.nan).std(axis=1, skipna=True)\n",
        "            cv_stds.append(temp)\n",
        "\n",
        "        for std,mean, group in zip(cv_stds, cv_means, groups):\n",
        "            temp = std/mean\n",
        "            temp.name=group\n",
        "            cvs.append(temp)\n",
        "        \n",
        "        cvs = pd.concat(cvs, axis=1)\n",
        "        self.cv = cvs\n",
        "        #print(cvs.head())  "
      ]
    }
  ]
}